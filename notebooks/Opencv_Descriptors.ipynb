{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from natsort import natsorted\n",
    "import time\n",
    "#uninstall opencv-python and install opencv-contrib-python, also install natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = '../data/rgb1/'\n",
    "img_path = '../data/rgb1/143.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_intensity(image, alpha, beta):\n",
    "    modified_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    return modified_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ORB(img_path, output_path='../results_data/Orb', show=False):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initiate ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    output_img = cv2.drawKeypoints(img, keypoints, None, color=(0, 0, 255))\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, os.path.basename(img_path))\n",
    "    cv2.imwrite(output_file, output_img)\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIFT(img_path, output_path='../results_data/Sift', show=False):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initiate SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    output_img = cv2.drawKeypoints(img, keypoints, None, color=(0, 0, 255))\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, os.path.basename(img_path))\n",
    "    cv2.imwrite(output_file, output_img)\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AKAZE(img_path, output_path='../results_data/Akaze', show=False):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initiate AKAZE detector\n",
    "    akaze = cv2.AKAZE_create()\n",
    "\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = akaze.detectAndCompute(gray, None)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    output_img = cv2.drawKeypoints(img, keypoints, None, color=(0, 0, 255))\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, os.path.basename(img_path))\n",
    "    cv2.imwrite(output_file, output_img)\n",
    "    \n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HarrisLaplace_BRIEF(img_path, output_path='../results_data/Harris_Laplace_Brief', show=False):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initiate FAST detector\n",
    "    star = cv2.xfeatures2d.HarrisLaplaceFeatureDetector.create()\n",
    "\n",
    "    # Initiate BRIEF extractor\n",
    "    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "    keypoints = star.detect(gray, None)\n",
    "    keypoints, descriptors = brief.compute(gray, keypoints)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    output_img = cv2.drawKeypoints(img, keypoints, None, color=(0, 0, 255))\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, os.path.basename(img_path))\n",
    "    cv2.imwrite(output_file, output_img)\n",
    "    \n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fast_FREAK(img_path, output_path='../results_data/Fast_Freak', show=False):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initiate FAST detector\n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "    # Initiate FREAK extractor\n",
    "    freak = cv2.xfeatures2d.FREAK_create()\n",
    "\n",
    "    # Detect keypoints\n",
    "    keypoints = fast.detect(gray, None)\n",
    "\n",
    "    # Compute descriptors\n",
    "    keypoints, descriptors = freak.compute(gray, keypoints)\n",
    "    \n",
    "    # Draw keypoints on the image\n",
    "    output_img = cv2.drawKeypoints(img, keypoints, None, color=(0, 0, 255))\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, os.path.basename(img_path))\n",
    "    cv2.imwrite(output_file, output_img)\n",
    "    \n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Orb_Brief(img_path, output_path='../results_data/Dense_Brief' ,show=False):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Initiate ORB detector with DENSE flag for dense keypoint detection\n",
    "    orb = cv2.ORB_create(10000, scoreType=cv2.ORB_FAST_SCORE)\n",
    "    orb.setFastThreshold(0)\n",
    "\n",
    "    # Find the keypoints with ORB\n",
    "    keypoints = orb.detect(gray, None)\n",
    "\n",
    "    # Initiate BRIEF extractor\n",
    "    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "    # Compute descriptors using BRIEF extractor\n",
    "    keypoints, descriptors = brief.compute(img, keypoints)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    output_img = cv2.drawKeypoints(img, keypoints, None, color=(0, 0, 255))\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, os.path.basename(img_path))\n",
    "    cv2.imwrite(output_file, output_img)\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fast_Brief(img_path, output_path='../results_data/Dense_Brief' ,show=False):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Initiate ORB detector with DENSE flag for dense keypoint detection\n",
    "    orb = cv2.ORB_create(10000, scoreType=cv2.ORB_FAST_SCORE)\n",
    "    orb.setFastThreshold(0)\n",
    "\n",
    "    # Find the keypoints with ORB\n",
    "    keypoints = orb.detect(gray, None)\n",
    "\n",
    "    # Initiate BRIEF extractor\n",
    "    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "    # Compute descriptors using BRIEF extractor\n",
    "    keypoints, descriptors = brief.compute(img, keypoints)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    output_img = cv2.drawKeypoints(img, keypoints, None, color=(0, 0, 255))\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, os.path.basename(img_path))\n",
    "    cv2.imwrite(output_file, output_img)\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fast_LATCH(img_path, output_path='../results_data/Fast_Latch', show=False):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initiate FAST detector\n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "    # Initiate LATCH extractor\n",
    "    latch = cv2.xfeatures2d.LATCH_create()\n",
    "\n",
    "    # Detect keypoints\n",
    "    keypoints = fast.detect(gray, None)\n",
    "\n",
    "    # Compute descriptors\n",
    "    keypoints, descriptors = latch.compute(gray, keypoints)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    output_img = cv2.drawKeypoints(img, keypoints, None, color=(0, 0, 255))\n",
    "\n",
    "    # Save output image in a new folder named \"LATCH\" (create the folder if it doesn't exist)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, os.path.basename(img_path))\n",
    "    cv2.imwrite(output_file, output_img)\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_file in natsorted([f for f in os.listdir(img_folder) if f.endswith('.png')]):\n",
    "#     image_path = os.path.join(img_folder, image_file)\n",
    "#     Dense_Brief(image_path)\n",
    "#     Fast_LATCH(image_path)\n",
    "#     HarrisLaplace_BRIEF(image_path)\n",
    "#     SIFT(image_path)\n",
    "#     ORB(image_path)\n",
    "#     Fast_FREAK(image_path)\n",
    "#     AKAZE(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two frames and convert them to grayscale\n",
    "frame1 = cv2.imread('../data/Rearrangment1/modified/frame1/rgb1/107.png')\n",
    "frame2 = cv2.imread('../data/Rearrangment1/original/rgb0/93.png')\n",
    "gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = modify_intensity(gray2, alpha=1, beta=0)\n",
    "\n",
    "# Initiate BRIEF descriptor extractor\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "# Initiate FAST detector for keypoints\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both frames\n",
    "keypoints1 = fast.detect(gray1, None)\n",
    "keypoints1, descriptors1 = brief.compute(gray1, keypoints1)\n",
    "\n",
    "keypoints2 = fast.detect(gray2, None)\n",
    "keypoints2, descriptors2 = brief.compute(gray2, keypoints2)\n",
    "\n",
    "# Match descriptors using a feature matching algorithm (e.g., Brute-Force matcher)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "# Apply ratio test to filter out bad matches\n",
    "good_matches = [m for m in matches if m.distance < 0.7 * min(m.queryIdx, m.trainIdx)]\n",
    "\n",
    "# Estimate the transformation (e.g., homography matrix) using RANSAC\n",
    "if len(good_matches) >= 4:\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Apply the transformation to verify the correctness\n",
    "transformed_frame1 = cv2.warpPerspective(frame1, M, (frame2.shape[1], frame2.shape[0]))\n",
    "\n",
    "# Display the transformed image\n",
    "cv2.imshow('Transformed Frame 1', transformed_frame1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two frames and convert them to grayscale\n",
    "frame1 = cv2.imread('../data/MH_02/1403636579763555584.png')\n",
    "frame2 = cv2.imread('../data/MH_02/1403636579763555584.png')\n",
    "gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = modify_intensity(gray2, alpha=1, beta=0)\n",
    "\n",
    "# Initiate BRIEF descriptor extractor\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "# Initiate FAST detector for keypoints\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both frames\n",
    "keypoints1 = fast.detect(gray1, None)\n",
    "keypoints1, descriptors1 = brief.compute(gray1, keypoints1)\n",
    "\n",
    "keypoints2 = fast.detect(gray2, None)\n",
    "keypoints2, descriptors2 = brief.compute(gray2, keypoints2)\n",
    "\n",
    "# Match descriptors using a feature matching algorithm (e.g., Brute-Force matcher)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "# Sort matches based on their distance\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Draw correspondences on the images\n",
    "correspondence_img = cv2.drawMatches(frame1, keypoints1, frame2, keypoints2, matches[:10], None)\n",
    "\n",
    "# Show the correspondences\n",
    "cv2.imshow('Correspondences', correspondence_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'matcher'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Match descriptors using a feature matching algorithm (e.g., Brute-Force matcher)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m bf \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mBFMatcher(cv2\u001b[39m.\u001b[39mNORM_HAMMING, crossCheck\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m bf \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mmatcher\n\u001b[1;32m     21\u001b[0m matches \u001b[39m=\u001b[39m bf\u001b[39m.\u001b[39mmatch(descriptors1, descriptors2)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Sort matches based on their distance\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'matcher'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two frames and convert them to grayscale\n",
    "frame1 = cv2.imread('../data/MH_02/1403636579763555584.png')\n",
    "frame2 = cv2.imread('../data/MH_02/1403636579763555584.png')\n",
    "gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = modify_intensity(gray2, alpha=1, beta=0)\n",
    "\n",
    "# Initiate ORB detector for keypoints\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both frames\n",
    "keypoints1, descriptors1 = orb.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "# Match descriptors using a feature matching algorithm (e.g., Brute-Force matcher)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "bf = cv2.matcher\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "# Sort matches based on their distance\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Draw correspondences on the images\n",
    "correspondence_img = cv2.drawMatches(frame1, keypoints1, frame2, keypoints2, matches[:10], None)\n",
    "\n",
    "# Show the correspondences\n",
    "cv2.imshow('Correspondences', correspondence_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough good matches for homography estimation.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two frames and convert them to grayscale\n",
    "frame1 = cv2.imread('../data/MH_02/1403636579763555584.png')\n",
    "frame2 = cv2.imread('../data/MH_02/1403636579763555584.png')\n",
    "gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initiate ORB detector and descriptor extractor\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both frames\n",
    "keypoints1, descriptors1 = orb.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "# Match descriptors using a feature matching algorithm (e.g., Brute-Force matcher)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "# Apply ratio test to filter out bad matches\n",
    "good_matches = [m for m in matches if m.distance < 0.75 * min([n.distance for n in matches])]\n",
    "\n",
    "# Check if we have enough good matches for homography estimation\n",
    "if len(good_matches) >= 4:\n",
    "    # Extract the matched keypoints' locations\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Apply RANSAC to find inliers and outliers\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Extract the inliers and outliers\n",
    "    inliers = [good_matches[i] for i in range(len(good_matches)) if mask[i]]\n",
    "    outliers = [good_matches[i] for i in range(len(good_matches)) if not mask[i]]\n",
    "\n",
    "    # Draw correspondences and highlight inliers and outliers on the images\n",
    "    correspondence_img = cv2.drawMatches(frame1, keypoints1, frame2, keypoints2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    correspondence_img_inliers = cv2.drawMatches(frame1, keypoints1, frame2, keypoints2, inliers, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS, matchColor=(0, 255, 0))\n",
    "    correspondence_img_outliers = cv2.drawMatches(frame1, keypoints1, frame2, keypoints2, outliers, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS, matchColor=(0, 0, 255))\n",
    "\n",
    "    # Show the correspondences, inliers, and outliers\n",
    "    cv2.imshow('Correspondences', correspondence_img)\n",
    "    cv2.imshow('Inliers', correspondence_img_inliers)\n",
    "    cv2.imshow('Outliers', correspondence_img_outliers)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"Not enough good matches for homography estimation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two frames and convert them to grayscale\n",
    "frame1 = cv2.imread('../data/MH_02/1403636579763555584.png')\n",
    "frame2 = cv2.imread('../data/MH_02/1403636579763555584.png')\n",
    "gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = modify_intensity(gray2, alpha=0.7, beta=-50)\n",
    "\n",
    "# Initiate SIFT detector and descriptor extractor\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both frames\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Match descriptors using a feature matching algorithm (e.g., Brute-Force matcher)\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test to filter out bad matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract the matched keypoints' locations\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Apply RANSAC to find inliers and outliers\n",
    "M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Extract the inliers and outliers\n",
    "inliers = [good_matches[i] for i in range(len(good_matches)) if mask[i]]\n",
    "outliers = [good_matches[i] for i in range(len(good_matches)) if not mask[i]]\n",
    "\n",
    "# Draw correspondences and highlight inliers and outliers on the images\n",
    "correspondence_img = cv2.drawMatches(frame1, keypoints1, frame2, keypoints2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "correspondence_img_inliers = cv2.drawMatches(frame1, keypoints1, frame2, keypoints2, inliers, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS, matchColor=(0, 255, 0))\n",
    "correspondence_img_outliers = cv2.drawMatches(frame1, keypoints1, frame2, keypoints2, outliers, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS, matchColor=(0, 0, 255))\n",
    "\n",
    "# Show the correspondences, inliers, and outliers\n",
    "cv2.imshow('Correspondences', correspondence_img)\n",
    "cv2.imshow('Inliers', correspondence_img_inliers)\n",
    "cv2.imshow('Outliers', correspondence_img_outliers)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     M_new_homo[:\u001b[39m3\u001b[39m, :\u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m M_new\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Update the transformation matrix\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     M \u001b[39m=\u001b[39m M_new_homo \u001b[39m@\u001b[39;49m M\n\u001b[1;32m     68\u001b[0m \u001b[39m# Apply the final transformation to verify the correctness\u001b[39;00m\n\u001b[1;32m     69\u001b[0m transformed_frame1 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mwarpPerspective(frame1, M, (frame2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], frame2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def apply_average_filter(img):\n",
    "    kernel = np.ones((3, 3), dtype=np.float32) / 9.0\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "# Load two images and convert them to grayscale\n",
    "frame1 = cv2.imread('../data/rgb1/26.png')\n",
    "frame2 = cv2.imread('../data/rgb1/27.png')\n",
    "gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply the average 3x3 filter to both images\n",
    "filtered_gray1 = apply_average_filter(gray1)\n",
    "filtered_gray2 = apply_average_filter(gray2)\n",
    "# filtered_gray1 = gray1\n",
    "# filtered_gray2 = gray2\n",
    "\n",
    "# Initiate SIFT detector and descriptor extractor\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(filtered_gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(filtered_gray2, None)\n",
    "\n",
    "# Match descriptors using a feature matching algorithm (e.g., Brute-Force matcher)\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test to filter out bad matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract the matched keypoints' locations\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Initialize the transformation matrix (identity matrix for the first iteration)\n",
    "M = np.eye(3, dtype=np.float32)\n",
    "\n",
    "# Iterative Closest Point (ICP) algorithm\n",
    "max_iterations = 50\n",
    "for i in range(max_iterations):\n",
    "    # Transform the source points using the current transformation matrix\n",
    "    transformed_pts = cv2.perspectiveTransform(src_pts, M)\n",
    "    \n",
    "    # Find the nearest neighbors in the target points for each transformed point\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(dst_pts.reshape(-1, 2))\n",
    "    distances, indices = nbrs.kneighbors(transformed_pts.reshape(-1, 2))\n",
    "    \n",
    "    # Get the corresponding points in the target cloud for each transformed point\n",
    "    matched_dst_pts = dst_pts[indices].reshape(-1, 1, 2)\n",
    "    \n",
    "    # Estimate the transformation using the matched points\n",
    "    M_new, _ = cv2.findHomography(src_pts, matched_dst_pts, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    # Convert 3x3 matrix to 4x4 homogeneous matrix\n",
    "    M_new_homo = np.eye(4, dtype=np.float32)\n",
    "    M_new_homo[:3, :3] = M_new\n",
    "    \n",
    "    # Update the transformation matrix\n",
    "    M = M_new_homo @ M\n",
    "\n",
    "# Apply the final transformation to verify the correctness\n",
    "transformed_frame1 = cv2.warpPerspective(frame1, M, (frame2.shape[1], frame2.shape[0]))\n",
    "\n",
    "# Display the transformed image\n",
    "cv2.imshow('Transformed Frame 1', transformed_frame1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39m# Update the transformation matrix (including translation components)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     M_new_inv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39minv(M_new_homo)\n\u001b[0;32m---> 65\u001b[0m     M \u001b[39m=\u001b[39m M_new_inv \u001b[39m@\u001b[39;49m M\n\u001b[1;32m     67\u001b[0m \u001b[39m# Apply the final transformation to verify the correctness\u001b[39;00m\n\u001b[1;32m     68\u001b[0m transformed_frame1 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mwarpPerspective(frame1, M, (frame2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], frame2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def apply_average_filter(img):\n",
    "    kernel = np.ones((3, 3), dtype=np.float32) / 9.0\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "# Load RGB images and corresponding depth maps (assuming they are in .npy format)\n",
    "frame1_rgb = cv2.imread('../data/rgb1/26.png')\n",
    "frame2_rgb = cv2.imread('../data/rgb1/27.png')\n",
    "\n",
    "frame1_depth = np.load('../data/depth1/26.npy')\n",
    "frame2_depth = np.load('../data/depth1/27.npy')\n",
    "\n",
    "gray1 = cv2.cvtColor(frame1_rgb, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(frame2_rgb, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply the average 3x3 filter to both images\n",
    "filtered_gray1 = apply_average_filter(gray1)\n",
    "filtered_gray2 = apply_average_filter(gray2)\n",
    "\n",
    "# Initiate SIFT detector and descriptor extractor\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(filtered_gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(filtered_gray2, None)\n",
    "\n",
    "# Match descriptors using a feature matching algorithm (e.g., Brute-Force matcher)\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test to filter out bad matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract the matched keypoints' locations\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Initialize the transformation matrix (identity matrix for the first iteration)\n",
    "M = np.eye(4, dtype=np.float32)\n",
    "\n",
    "# Iterative Closest Point (ICP) algorithm\n",
    "max_iterations = 50\n",
    "for i in range(max_iterations):\n",
    "    # Transform the source points using the current transformation matrix\n",
    "    transformed_pts = cv2.perspectiveTransform(src_pts, M[:3, :3])\n",
    "    transformed_pts_depth = np.array([frame1_depth[int(pt[0][1]), int(pt[0][0])] for pt in transformed_pts])\n",
    "    transformed_pts_3d = np.hstack((transformed_pts.reshape(-1, 2), transformed_pts_depth.reshape(-1, 1)))\n",
    "    \n",
    "    # Find the nearest neighbors in the target points for each transformed point\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(dst_pts.reshape(-1, 2))\n",
    "    distances, indices = nbrs.kneighbors(transformed_pts_3d[:, :2])\n",
    "    \n",
    "    # Get the corresponding points in the target cloud for each transformed point\n",
    "    matched_dst_pts = dst_pts[indices].reshape(-1, 1, 2)\n",
    "    \n",
    "    # Estimate the transformation using the matched points\n",
    "    M_new, _ = cv2.findHomography(src_pts, matched_dst_pts, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    # Convert 3x3 matrix to 4x4 homogeneous matrix\n",
    "    M_new_homo = np.eye(4, dtype=np.float32)\n",
    "    M_new_homo[:3, :3] = M_new\n",
    "    \n",
    "    # Update the transformation matrix (including translation components)\n",
    "    M = M @ np.linalg.inv(M_new_homo)\n",
    "\n",
    "# Apply the final transformation to verify the correctness\n",
    "transformed_frame1 = cv2.warpPerspective(frame1_rgb, M, (frame2_rgb.shape[1], frame2_rgb.shape[0]))\n",
    "\n",
    "# Display the transformed image\n",
    "cv2.imshow('Transformed Frame 1', transformed_frame1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m Dense_Brief(image_path, output_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../results_data/MH_01/Dense_Brief\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m Fast_LATCH(image_path, output_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../results_data/MH_01/Dense_Fast_LATCHBrief\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m HarrisLaplace_BRIEF(image_path, output_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../results_data/MH_01/HarrisLaplace_BRIEF\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m SIFT(image_path, output_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../results_data/MH_01/SIFT\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m ORB(image_path, output_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../results_data/MH_01/ORB\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m, in \u001b[0;36mHarrisLaplace_BRIEF\u001b[0;34m(img_path, output_path, show)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Initiate BRIEF extractor\u001b[39;00m\n\u001b[1;32m      9\u001b[0m brief \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mxfeatures2d\u001b[39m.\u001b[39mBriefDescriptorExtractor_create()\n\u001b[0;32m---> 11\u001b[0m keypoints \u001b[39m=\u001b[39m star\u001b[39m.\u001b[39;49mdetect(gray, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     12\u001b[0m keypoints, descriptors \u001b[39m=\u001b[39m brief\u001b[39m.\u001b[39mcompute(gray, keypoints)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Draw keypoints on the image\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img_folder = '../data/MH_02/'\n",
    "for image_file in natsorted([f for f in os.listdir(img_folder) if f.endswith('.png')]):\n",
    "    image_path = os.path.join(img_folder, image_file)\n",
    "    Dense_Brief(image_path, output_path='../results_data/MH_01/Dense_Brief')\n",
    "    Fast_LATCH(image_path, output_path='../results_data/MH_01/Dense_Fast_LATCHBrief')\n",
    "    HarrisLaplace_BRIEF(image_path, output_path='../results_data/MH_01/HarrisLaplace_BRIEF')\n",
    "    SIFT(image_path, output_path='../results_data/MH_01/SIFT')\n",
    "    ORB(image_path, output_path='../results_data/MH_01/ORB')\n",
    "    Fast_FREAK(image_path, output_path='../results_data/MH_01/Fast_FREAK')\n",
    "    AKAZE(image_path, output_path='../results_data/MH_01/AKAZE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
